import json
import sys
import random
from datetime import datetime, timezone
from textwrap import dedent
from flask import Flask, request, jsonify

from pydantic import BaseModel, Field
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import PydanticOutputParser
from dotenv import load_dotenv

from typing import List
import os
from langchain_groq import ChatGroq

# --- 1. SETUP ---
load_dotenv(override=True)
llm = ChatGroq(
    model="llama-3.1-8b-instant", temperature=0, api_key=os.getenv("GROQ_API_KEY")
)

# --- 2. DEFINE STRUCTURED OUTPUT FOR LLM INSIGHTS ---


class ActionableItem(BaseModel):
    """A single, concrete action item for a manager."""

    recommendation: str = Field(
        description="A concrete, actionable step for the manager (e.g., 'Schedule a 1-on-1 with Prakhar Shukla')."
    )
    rationale: str = Field(
        description="The reason this action is recommended, based on the provided data (e.g., 'His tasks are blocked and he may need support.')."
    )


class LLMReportEnhancements(BaseModel):
    """The structured insights generated by the LLM."""

    executive_summary: str = Field(
        description="A concise, high-level summary of the project's status, risks, and outlook for the reporting manager."
    )
    risks_and_bottlenecks: List[str] = Field(
        description="A list of identified risks, potential delays, or team members who might be overloaded or blocked."
    )
    suggested_action_items: List[ActionableItem] = Field(
        description="A list of recommended actions for the manager, including specific interactions with employees."
    )
    predicted_completion_outlook: str = Field(
        description="A qualitative forecast of the project's completion (e.g., 'On track for timely completion', 'At risk of minor delays')."
    )


# --- 3. HELPER FUNCTIONS ---


def load_json_file(filepath: str):
    """Safely loads a JSON file."""
    try:
        with open(filepath, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        print(
            f"Error: The file '{filepath}' was not found. Please run the main agent first."
        )
        sys.exit(1)
    except json.JSONDecodeError:
        print(f"Error: The file '{filepath}' contains invalid JSON.")
        sys.exit(1)


def get_all_subordinates(employee_node: dict) -> list:
    """Recursively gets a flat list of all people in a subtree, including the manager."""
    members = [employee_node["name"]]
    for subordinate in employee_node.get("subordinates", []):
        members.extend(get_all_subordinates(subordinate))
    return members


def find_employee_node(employee_name: str, hierarchy_node: dict) -> dict | None:
    """Recursively finds the full dictionary object for an employee."""
    if hierarchy_node["name"].lower() == employee_name.lower():
        return hierarchy_node
    for subordinate in hierarchy_node.get("subordinates", []):
        found = find_employee_node(employee_name, subordinate)
        if found:
            return found
    return None


def enhance_report_with_llm(report_data: dict) -> dict:
    """
    Uses an LLM to analyze the raw report data and generate actionable insights.
    """
    parser = PydanticOutputParser(pydantic_object=LLMReportEnhancements)
    data_context = json.dumps(report_data, indent=2)
    # --- FIX IS HERE: MORE ROBUST PROMPT WITH A FEW-SHOT EXAMPLE ---
    prompt = dedent(
        f"""
        You are an expert Project Management Analyst AI. Your task is to analyze a raw JSON project status report and generate insightful, actionable commentary for the manager.

        **CONTEXT:**
        The report is for a project at Scaler, an ed-tech company. The manager needs to understand project health, identify risks, and know what actions to take.

        **RAW PROJECT DATA:**
        ```json
        {data_context}
        ```

        **YOUR ANALYSIS TASK:**
        Based on the raw data, provide an analysis covering an executive summary, risks, suggested actions, and a completion outlook.

        **--- EXAMPLE OF THE DESIRED OUTPUT FORMAT ---**
        ```json
        {{
          "executive_summary": "The project is currently at 25% completion but is facing some risks due to blocked tasks. While some team members are progressing well, targeted interventions are needed to maintain the timeline.",
          "risks_and_bottlenecks": [
            "Overall progress is slow, indicating a potential for delay.",
            "Jane Doe's task is 'Blocked', which could impact downstream dependencies.",
            "John Smith has a low completion rate of 0%, suggesting he might be stuck or overloaded."
          ],
          "suggested_action_items": [
            {{
              "recommendation": "Schedule a 1-on-1 check-in with Jane Doe.",
              "rationale": "Her task is marked as 'Blocked'. It's crucial to understand the obstacle and provide support to get her unblocked."
            }},
            {{
              "recommendation": "Publicly praise Mary Major for her 100% completion rate.",
              "rationale": "Recognizing high-performers boosts morale and sets a positive example for the team."
            }}
          ],
          "predicted_completion_outlook": "At risk of minor delays if bottlenecks are not addressed promptly."
        }}
        ```
        **--- END OF EXAMPLE ---**

        **CRITICAL INSTRUCTIONS:**
        1.  Your entire response MUST be a single, valid JSON object, exactly like the example above.
        2.  Do NOT include any introductory text, explanations, or markdown formatting (like ```json) outside of the JSON object itself.
        3.  Do NOT return a JSON Schema. Your output must be a clean JSON instance, not a definition containing keys like '$defs' or 'properties'.

        {parser.get_format_instructions()}
    """
    )
    # --- END OF FIX ---

    chain = llm | parser
    try:
        insights = chain.invoke(prompt)
        return insights.dict()
    except Exception as e:
        print(f"\nError during LLM enhancement: {e}")
        return {
            "executive_summary": "Error: Could not generate AI insights.",
            "risks_and_bottlenecks": [],
            "suggested_action_items": [],
            "predicted_completion_outlook": "Unknown due to error.",
        }


def generate_report(
    requester_name: str,
    project_name_query: str,
    hierarchy_data: dict,
    project_data: dict,
):
    """
    Generates a frontend-friendly progress report, now enhanced with LLM insights.
    """
    target_project = None
    for project_name, project_details in project_data.items():
        if project_name_query.lower() in project_name.lower():
            target_project = project_details
            break
    if not target_project:
        print(
            f"Error: No project found matching '{project_name_query}'. Available: {list(project_data.keys())}"
        )
        return

    requester_node = find_employee_node(requester_name, hierarchy_data)
    if not requester_node:
        print(f"Error: Employee '{requester_name}' not found.")
        return

    subtree_members = get_all_subordinates(requester_node)

    tasks_in_subtree = []
    for task in target_project.get("tasks", []):
        if task["assignee_name"] in subtree_members:
            if task.get("status") == "Assigned":
                task["status"] = random.choice(
                    ["Assigned", "In Progress", "Completed", "Blocked"]
                )
            tasks_in_subtree.append(task)

    team_progress_breakdown = {}
    total_completed_tasks = 0
    for task in tasks_in_subtree:
        assignee = task["assignee_name"]
        if assignee not in team_progress_breakdown:
            employee_node = find_employee_node(assignee, hierarchy_data)
            team_progress_breakdown[assignee] = {
                "member_name": assignee,
                "job_role": employee_node.get("job_role", "N/A"),
                "total_tasks": 0,
                "completed_tasks": 0,
                "assigned_tasks": [],
            }
        team_progress_breakdown[assignee]["total_tasks"] += 1
        if task["status"] == "Completed":
            team_progress_breakdown[assignee]["completed_tasks"] += 1
            total_completed_tasks += 1
        team_progress_breakdown[assignee]["assigned_tasks"].append(
            {
                "task_id": task["task_id"],
                "task_title": task["task_title"],
                "status": task["status"],
            }
        )

    final_breakdown_list = []
    for member_data in team_progress_breakdown.values():
        total = member_data["total_tasks"]
        completed = member_data["completed_tasks"]
        member_data["completion_rate"] = (
            round((completed / total) * 100, 2) if total > 0 else 0
        )
        final_breakdown_list.append(member_data)

    total_tasks_count = len(tasks_in_subtree)
    overall_progress = (
        round((total_completed_tasks / total_tasks_count) * 100, 2)
        if total_tasks_count > 0
        else 0
    )

    data_for_llm = {
        "report_metadata": {
            "project_name": target_project["project_name"],
            "report_for": requester_name,
        },
        "project_summary": {
            "total_tasks_in_subtree": total_tasks_count,
            "completed_tasks_in_subtree": total_completed_tasks,
            "progress_percentage": overall_progress,
        },
        "team_progress_breakdown": sorted(
            final_breakdown_list, key=lambda x: x["member_name"]
        ),
    }

    print("\nðŸ¤– Analyzing data and generating AI insights...")
    llm_insights = enhance_report_with_llm(data_for_llm)

    final_report = {**data_for_llm, "llm_insights": llm_insights}
    final_report["project_summary"]["overall_goal"] = target_project["overall_goal"]
    final_report["report_metadata"]["generated_at"] = datetime.now(
        timezone.utc
    ).isoformat()

    return final_report


if __name__ == "__main__":
    if len(sys.argv) != 3:
        print(
            'Usage: python generate_report.py "<Employee Name>" "<Project Name Query>"'
        )
        print('Example: python generate_report.py "Naman Bhalla" "DSML Placement"')
        sys.exit(1)

    requester_name_arg = sys.argv[1]
    project_name_query_arg = sys.argv[2]
    # print(sys.argv)
    # exit()

    print("Loading data from generated files...")
    hierarchy = load_json_file("final_team_assignments.json")
    projects = load_json_file("project_report_data.json")

    print(
        f"Generating report for '{requester_name_arg}' on projects matching '{project_name_query_arg}'..."
    )
    report = generate_report(
        requester_name_arg, project_name_query_arg, hierarchy, projects
    )

    # return report


def save_report_to_file(report):
    if report:
        report_json = json.dumps(report, indent=2)
        report_filename = "report_output.json"
        with open(report_filename, "w") as f:
            f.write(report_json)
        return True
    return False


# Initialize Flask app
app = Flask(__name__)


@app.route("/generate-report", methods=["POST"])
def generate_report_endpoint():
    data = request.get_json()
    requester_name = data.get("requester_name")
    project_name = data.get("project_name")

    if not requester_name or not project_name:
        return (
            jsonify({"error": "Both requester_name and project_name are required"}),
            400,
        )

    try:
        report = generate_report(requester_name)
        if report:
            return jsonify(
                {
                    "requester_name": requester_name,
                    "project_name": project_name,
                    "report": report,
                }
            )
        else:
            return jsonify({"error": "Failed to generate report"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    app.run(debug=True, port=5000)
